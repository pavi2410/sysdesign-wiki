---
name: Google Photos
tagline: AI-powered photo storage, search, and memories for billions of images
category: productivity
tags: [photos, storage, AI, search, media, google, ML]
---
import { Mermaid, Scale, ComponentList, ComponentCard, Requirements, Tradeoff } from '@/components/mdx';

## Overview

Google Photos is a photo and video storage service used by 1B+ people, storing over 4 trillion photos. Its architecture uniquely combines massive media storage with deep AI/ML capabilities — automatic organization by people, places, and things; natural language search ("dog on beach"); and AI-generated creations (collages, animations, memories). The system must handle high-throughput media uploads, on-the-fly transcoding for diverse device displays, efficient deduplication, and ML inference at scale for every uploaded photo. Google Photos is one of the largest deployments of computer vision in the world.

## Scale

<Scale items={{
  "Users": "1B+",
  "Photos stored": "4T+",
  "Photos uploaded per day": "1.2B+",
  "ML models run per photo": "10+",
}} />

## Requirements

<Requirements
  functional={[
    'Photo and video upload with automatic backup',
    'AI-powered search (people, objects, scenes, text in images)',
    'Automatic albums and Memories',
    'Sharing and collaborative albums',
    'Photo editing with AI enhancements (Magic Eraser, etc.)',
    'On-device and cloud-based ML processing',
    'Export and Google Takeout integration',
  ]}
  nonFunctional={[
    'High durability — zero photo loss (11 nines)',
    'Fast upload even on slow connections (adaptive quality)',
    'Sub-second search across billions of personal photos',
    'Low-latency thumbnail and preview serving',
    'Efficient storage — smart compression and deduplication',
    'Privacy — ML inference without human review of photos',
  ]}
/>

## High-Level Architecture

<Mermaid chart={`graph TB
    subgraph Clients
        ANDROID[Android App]
        IOS[iOS App]
        WEB[Web App]
    end
    subgraph Edge["Edge / CDN"]
        CDN[Google CDN]
        LB[Load Balancer]
    end
    subgraph Services["Core Services"]
        UPLOAD[Upload Service]
        MEDIA[Media Processing]
        ML[ML Pipeline]
        SEARCH[Search Service]
        ALBUM[Album Service]
        MEMORY[Memories Engine]
        SHARE[Sharing Service]
    end
    subgraph MLModels["ML Models"]
        FACE[Face Recognition]
        OBJ[Object Detection]
        OCR[OCR]
        GEO[Geo Classifier]
        QUALITY[Quality Assessment]
    end
    subgraph Storage["Storage"]
        BLOB[(Media Blob Store)]
        META[(Metadata DB)]
        FINDEX[(Face Index)]
        VINDEX[(Visual Feature Index)]
    end
    ANDROID & IOS & WEB --> CDN & LB
    LB --> UPLOAD & SEARCH & ALBUM & SHARE
    UPLOAD --> MEDIA
    MEDIA --> BLOB
    MEDIA --> ML
    ML --> FACE & OBJ & OCR & GEO & QUALITY
    FACE --> FINDEX
    OBJ --> VINDEX
    SEARCH --> META & FINDEX & VINDEX
    ALBUM --> META
    MEMORY --> META & FINDEX
    CDN --> BLOB`} caption="Google Photos — High-level system architecture" />

## Key Components

<ComponentList>
<ComponentCard name="Upload Service">
Handles photo/video upload with resumable uploads and adaptive quality. Deduplicates uploads using perceptual hashing (not just content hash) to detect near-identical photos. Queues uploaded media for processing.
</ComponentCard>
<ComponentCard name="Media Processing Pipeline">
Transcodes uploaded media into multiple formats and resolutions — thumbnails (256px), previews (1024px), and full resolution. Videos are transcoded to H.264/VP9. Uses content-aware compression to reduce storage while maintaining visual quality.
</ComponentCard>
<ComponentCard name="ML Pipeline">
Runs 10+ ML models on every uploaded photo: face detection/recognition, object/scene classification, OCR, geolocation estimation from visual features, aesthetic quality scoring, and explicit content detection. Processes 1.2B+ photos per day.
</ComponentCard>
<ComponentCard name="Face Recognition">
Detects faces in photos and clusters them into identity groups using deep metric learning (face embeddings). Users can label clusters with names, enabling "photos of Alice" search. Uses ArcFace-style embeddings with a per-user face index.
</ComponentCard>
<ComponentCard name="Search Service">
Supports natural language queries ("sunset at the beach", "birthday cake 2023") by mapping query terms to visual concepts, detected objects, OCR text, and metadata (date, location). Combines text matching with visual feature similarity search.
</ComponentCard>
<ComponentCard name="Memories Engine">
Generates personalized "Memories" (e.g., "This day 3 years ago") by analyzing date patterns, location clustering, face groups, and aesthetic quality scores. Filters out low-quality, duplicate, or sensitive photos.
</ComponentCard>
<ComponentCard name="Sharing Service">
Manages photo sharing, shared albums, and partner sharing (automatic sharing of photos containing specific people). Shared albums have their own permission model independent of Google Drive.
</ComponentCard>
</ComponentList>

## Data Model

<Mermaid chart={`erDiagram
    PHOTO {
        string photo_id PK
        string user_id FK
        string blob_path
        string content_hash
        int width
        int height
        string mime_type
        bigint size_bytes
        timestamp taken_at
        float latitude
        float longitude
        string camera_model
    }
    FACE_CLUSTER {
        string cluster_id PK
        string user_id FK
        string label
        blob centroid_embedding
    }
    FACE_DETECTION {
        string detection_id PK
        string photo_id FK
        string cluster_id FK
        json bounding_box
        blob embedding
    }
    ALBUM {
        string album_id PK
        string user_id FK
        string title
        enum type
        timestamp created_at
    }
    ALBUM_ITEM {
        string album_id FK
        string photo_id FK
        int position
    }
    VISUAL_LABEL {
        string photo_id FK
        string label
        float confidence
    }
    PHOTO ||--o{ FACE_DETECTION : contains
    FACE_CLUSTER ||--o{ FACE_DETECTION : groups
    ALBUM ||--o{ ALBUM_ITEM : has
    PHOTO ||--o{ ALBUM_ITEM : in
    PHOTO ||--o{ VISUAL_LABEL : tagged_with`} caption="Google Photos — Entity relationship diagram" />

## ML Pipeline at Upload Scale

Every photo uploaded to Google Photos runs through a **multi-model ML pipeline** — one of the largest continuous inference workloads in the world.

**Pipeline stages**: When a photo is uploaded, it's processed through:
1. **Face detection**: Locates all faces and extracts 128-dim embeddings
2. **Face clustering**: Matches embeddings against the user's existing face clusters
3. **Object/scene classification**: Labels the photo with 10,000+ visual concepts
4. **OCR**: Extracts text from screenshots, signs, documents
5. **Geo estimation**: Predicts location from visual features when GPS is unavailable
6. **Quality scoring**: Rates aesthetic quality for Memories curation
7. **Safety classification**: Detects explicit or harmful content

**Batch inference**: Models run on TPU pods. Photos are batched (32-128 per batch) for efficient GPU/TPU utilization. The entire pipeline completes within seconds of upload.

**On-device ML**: Recent versions run face detection and basic classification on-device before upload, enabling features like smart album suggestions and faster initial indexing.

<Mermaid chart={`graph LR
    PHOTO[Uploaded Photo] --> FACE[Face Detection]
    FACE --> CLUSTER[Face Clustering]
    PHOTO --> OBJ[Object Detection]
    PHOTO --> OCR_M[OCR]
    PHOTO --> GEO[Geo Estimation]
    PHOTO --> QUAL[Quality Scoring]
    PHOTO --> SAFETY[Safety Check]
    CLUSTER --> INDEX[User Face Index]
    OBJ --> LABELS[Visual Labels]
    OCR_M --> TEXT[Text Index]
    QUAL --> SCORE[Curation Score]`} caption="ML Pipeline at Upload Scale" />

## Perceptual Hashing and Deduplication

With 1.2B+ photos uploaded daily, many are duplicates or near-duplicates (same photo at different resolutions, with slight edits, or re-shared). Google Photos uses **perceptual hashing** for deduplication.

**Content hash vs perceptual hash**: A cryptographic hash (SHA-256) only catches exact byte-identical files. A perceptual hash produces similar hashes for visually similar images, catching:
- Same photo at different resolutions
- Photos with minor color/exposure adjustments
- Screenshots of the same content
- Re-compressed versions of the same image

**Algorithm**: The perceptual hash pipeline resizes the image to a canonical size, converts to grayscale, applies a DCT (Discrete Cosine Transform), and quantizes the result into a compact binary hash. Two photos with a Hamming distance below a threshold are considered duplicates.

**Storage savings**: Deduplication at this scale saves petabytes of storage. When a duplicate is detected, only the metadata is stored — the blob references the existing stored copy.

## Natural Language Photo Search

Google Photos lets users search their library with natural language queries like "dog on beach" or "birthday cake" — no manual tagging required.

**Multi-modal search**: The search system combines multiple signals:
- **Visual labels**: Object/scene classifications from the ML pipeline (10,000+ concepts)
- **Face clusters**: Named face groups enable "photos of Alice"
- **OCR text**: Text extracted from images (screenshots, documents, signs)
- **Location**: Reverse geocoded place names ("photos in Paris")
- **Date parsing**: "photos from last summer" is parsed into a date range
- **EXIF metadata**: Camera model, orientation, etc.

**Embedding-based retrieval**: For queries that don't map to a known label, Google Photos uses CLIP-style visual-text embeddings. The query text is embedded into the same vector space as photo features, and nearest-neighbor search finds relevant photos.

**Personalization**: Search results are ranked by recency, quality score, and user interaction patterns (frequently viewed photos rank higher). The ranking model is per-user to reflect personal relevance.

## Architectural Tradeoffs

<Tradeoff
  decision="Server-side ML pipeline over client-only processing"
  pros={['Consistent results across all devices', 'Access to powerful TPU infrastructure', 'Models can be updated without app updates']}
  cons={['Requires uploading all photos to cloud', 'Processing cost per photo at 1.2B/day scale', 'Privacy concerns with server-side photo analysis']}
/>

<Tradeoff
  decision="Perceptual hashing over exact deduplication only"
  pros={['Catches near-duplicates — much higher dedup ratio', 'Saves petabytes of storage', 'Better user experience — fewer duplicate photos in library']}
  cons={['False positives possible for visually similar but different photos', 'Higher computation cost per upload', 'Perceptual hash collisions for certain image types']}
/>

<Tradeoff
  decision="Adaptive quality compression over lossless storage"
  pros={['Dramatically reduces storage costs', 'Imperceptible quality loss for most photos', 'Enables unlimited storage tiers']}
  cons={['Irreversible quality reduction', 'Professional photographers may notice compression artifacts', 'User trust issue — "Google changed my photos"']}
/>
